# Benchmark Results — v0.2.1

**Date:** 2026-02-20  
**Tag:** v0.2.1 (in progress, bug fixes pending)  
**Base Version:** v0.2.0 (Phase 1-4: Core LRU with concurrency and validation)

---

## Executive Summary

| Metric | Result | Verdict |
|--------|--------|---------|
| Peak throughput | 25.2M ops/s (readHeavy, 4T) | Good |
| Thread scaling | **Negative** (2T slower than 1T) | Critical issue |
| Write scalability | 17% efficiency at 4 threads | Poor |
| Correctness | Race condition in `get()` | Bug found |

**Key Finding:** The cache does not scale with thread count. Lock contention causes 50-60% throughput loss when moving from 1 to 2 threads. A fundamental redesign of the locking strategy is needed.

---

## Benchmark Design Philosophy

### Goals

The benchmark suite is designed to answer three questions:

1. **How does throughput scale with thread count?** — Does adding threads improve ops/sec, or does lock contention dominate?
2. **How do workload patterns affect performance?** — Read-heavy vs write-heavy vs mixed.
3. **Where are the bottlenecks?** — Lock contention, cache misses, or algorithmic overhead.

### Why JMH?

JMH (Java Microbenchmark Harness) is used because:

- **Warmup handling** — JIT compilation artifacts are excluded from measurements
- **Statistical rigor** — Multiple iterations with variance reporting
- **Dead code elimination prevention** — `Blackhole` consumes results to prevent JIT optimization
- **Thread coordination** — Built-in support for multi-threaded benchmarks

### Workload Design Rationale

| Workload | Read % | Write % | Key Distribution | Purpose |
|----------|--------|---------|------------------|---------|
| `readHeavy` | 100% | 0% | Within capacity (0–9,999) | Measure read throughput, relaxed LRU effectiveness |
| `writeHeavy` | 0% | 100% | Beyond capacity (0–99,999) | Stress eviction path, expose lock contention |
| `mixed` | 50% | 50% | Both distributions | Realistic workload, reader-writer interaction |

**Key space rationale:**
- `readHeavy`: Keys 0–9,999 (all pre-loaded, 100% hit rate) — isolates read path performance
- `writeHeavy`: Keys 0–99,999 with capacity 10,000 — guarantees evictions on ~90% of writes

---

## Cache Design Under Test

### Locking Strategy

```
┌─────────────────────────────────────────────────────────┐
│                    LRUCache Design                      │
├─────────────────────────────────────────────────────────┤
│  mapLock (ReentrantLock)   │  listLock (ReentrantLock)  │
│  - Protects HashMap        │  - Protects DoublyLinkedList│
│  - Always acquired first   │  - Acquired second         │
├─────────────────────────────────────────────────────────┤
│  get():  mapLock → unlock → tryLock(listLock) → unlock  │
│  put():  mapLock → listLock → unlock both               │
└─────────────────────────────────────────────────────────┘
```

### Relaxed LRU Semantics

Reads use `tryLock()` for recency updates:
- If `listLock` is available → move node to head
- If `listLock` is contended → skip recency update (best-effort)

Trade-off: Throughput over perfect LRU ordering.

---

## Benchmark Results

### Configuration

```
JMH Version: 1.37
JVM: JDK 25.0.2, OpenJDK 64-Bit Server VM
Platform: Apple Silicon (ARM64)
Warmup: 2-3 iterations × 2s
Measurement: 3 iterations × 3s
Fork: 0 (in-process, for quick iteration)
```

### Throughput (ops/sec) — Thread Scaling

| Benchmark | 1 Thread | 2 Threads | 4 Threads | 8 Threads |
|-----------|----------|-----------|-----------|-----------|
| `readHeavy` | **24.2M** | 12.6M | **25.2M** | 13.3M |
| `writeHeavy` | **14.9M** | 6.6M | 10.2M | 9.3M |
| `mixed` | **19.7M** | 9.4M | 12.2M | 11.7M |

### Thread Scaling Visualization

```
Throughput (M ops/s)
     │
  25 ┤ ●────────────────────●                    readHeavy
     │  \                  / \
  20 ┤   \                /   \
     │    ●              /     \                 mixed
  15 ┤     \   ●────────●       ●────●
     │      \ /                       \
  10 ┤       ●                         ●────●   writeHeavy
     │
   5 ┤
     │
   0 └────┬────┬────┬────┬────
          1    2    4    8   threads
```

### Key Observations

1. **Single-threaded is fastest for writes** — 14.9M ops/s, no lock contention
2. **2 threads causes massive regression** — throughput drops 50-60% across all workloads
3. **4 threads partially recovers for reads** — relaxed `tryLock()` helps readHeavy
4. **8 threads shows no improvement** — contention dominates, no scaling benefit

### Scaling Efficiency

| Benchmark | Ideal 4x (from 1T) | Actual 4T | Efficiency |
|-----------|-------------------|-----------|------------|
| `readHeavy` | 96.8M | 25.2M | **26%** |
| `writeHeavy` | 59.6M | 10.2M | **17%** |
| `mixed` | 78.8M | 12.2M | **15%** |

---

## Issues Discovered

### Issue 1: Race Condition in `get()` — Orphaned Node in DLL

**Symptom:** Invariant violation — node exists in DLL but not in map.

**Root Cause Analysis:**

```
Thread A (get)                    Thread B (put with eviction)
─────────────────                 ────────────────────────────
1. mapLock.lock()
2. node = map.get(key)
3. mapLock.unlock()
   ↓ WINDOW OPENS
                                  1. mapLock.lock()
                                  2. listLock.lock()
                                  3. removeTail() → detaches node
                                     (node.prev = null, node.next = null)
                                  4. map.remove(node.key)
                                  5. listLock.unlock()
                                  6. mapLock.unlock()
   ↓ WINDOW CLOSES
4. listLock.tryLock() ✓
5. moveNodeToHead(node)
   └─ detach(node) → early return (prev is null)
   └─ attachNodeToHead(node) → ATTACHES EVICTED NODE!
6. listLock.unlock()

RESULT: Node is in DLL but NOT in map → invariant broken
```

**Why it happens:**

1. `get()` releases `mapLock` before acquiring `listLock` — this gap allows eviction
2. `detach()` silently returns if `node.prev == null` (already detached)
3. `attachNodeToHead()` doesn't check if node is still valid (in map)

**Impact:**
- DLL size diverges from map size
- Memory leak (orphaned nodes never evicted)
- `assertInvariants()` fails under concurrent stress

### Issue 2: Potential Throughput Degradation Under Write-Heavy Load

**Hypothesis:** The `put()` method holds both locks for the entire operation:

```java
mapLock.lock();
try {
    listLock.lock();
    try {
        // entire put logic here
    } finally {
        listLock.unlock();
    }
} finally {
    mapLock.unlock();
}
```

This serializes all writes and blocks reads during eviction.

**Expected behavior:**
- `readHeavy` should scale well (relaxed locking)
- `writeHeavy` should show poor scaling (full serialization)
- `mixed` will be bottlenecked by writes

---

## Initial Interpretation

### Finding 1: Severe Lock Contention

The benchmark data reveals a critical problem: **adding threads makes performance worse**.

```
1 thread  → 2 threads:  -50% throughput (contention penalty)
2 threads → 4 threads:  +60% throughput (partial recovery)
4 threads → 8 threads:  -47% throughput (contention dominates again)
```

**Root cause:** Both `mapLock` and `listLock` are coarse-grained. Every operation must acquire at least one lock, and writes hold both locks for their entire duration.

### Finding 2: Relaxed LRU Helps Reads But Not Enough

The `readHeavy` workload recovers at 4 threads (25.2M ops/s, matching single-threaded) because:
- `tryLock()` allows reads to skip recency updates when contended
- Reads only need `mapLock` briefly

But this optimization introduces a **correctness bug** (see Issue 1 below).

### Finding 3: Write-Heavy is Fundamentally Bottlenecked

`writeHeavy` shows the worst scaling:
- 1T: 14.9M → 4T: 10.2M → 8T: 9.3M
- **Adding threads reduces throughput**

This is because `put()` holds both locks:
```java
mapLock.lock();
  listLock.lock();
    // entire put logic (map lookup, eviction, DLL update)
  listLock.unlock();
mapLock.unlock();
```

All writes are effectively **serialized**.

### Why Relaxed LRU is Not Enough

The relaxed LRU optimization (`tryLock` on reads) improves read throughput but introduces a subtle bug:

1. The design assumes the node fetched from the map remains valid
2. But releasing `mapLock` before `listLock` creates a window where the node can be evicted
3. The evicted node is then reattached to the DLL without being in the map

### Fundamental Trade-off Exposed

```
         Correctness ◄─────────────────────► Throughput
              │                                    │
              │   Current design sits here         │
              │              ↓                     │
              │         [BROKEN]                   │
              │                                    │
    "Hold both locks"                    "Release mapLock early"
    (safe but slow)                      (fast but racy)
```

### Possible Fixes (To Be Evaluated)

| Approach | Correctness | Throughput | Complexity |
|----------|-------------|------------|------------|
| Hold `mapLock` through entire `get()` | ✅ Safe | ❌ Poor | Low |
| Check node validity after acquiring `listLock` | ✅ Safe | ⚠️ Moderate | Medium |
| Use `ConcurrentHashMap` + lock-free DLL | ✅ Safe | ✅ Good | High |
| Mark nodes as "evicted" (soft delete) | ✅ Safe | ⚠️ Moderate | Medium |
| Striped locking (partition keyspace) | ✅ Safe | ✅ Good | High |

---

## Next Steps

1. **Run benchmarks** and fill in actual numbers
2. **Fix Issue 1** — validate node before `moveNodeToHead()`
3. **Re-benchmark** after fix to measure overhead
4. **Document trade-off** — how much throughput was sacrificed for correctness?

---

## Commands to Reproduce

```bash
# Build and run all benchmarks (4 threads, throughput mode)
cd cache-engine
mvn clean test-compile exec:java

# Thread scaling test (readHeavy)
mvn test-compile exec:exec -Dexec.args="-f 1 -wi 3 -i 3 -t 1 readHeavy"
mvn test-compile exec:exec -Dexec.args="-f 1 -wi 3 -i 3 -t 2 readHeavy"
mvn test-compile exec:exec -Dexec.args="-f 1 -wi 3 -i 3 -t 4 readHeavy"
mvn test-compile exec:exec -Dexec.args="-f 1 -wi 3 -i 3 -t 8 readHeavy"

# Latency mode (microseconds)
mvn test-compile exec:exec -Dexec.args="-f 1 -wi 3 -i 3 -t 4 -bm sampletime -tu us readHeavy"
```
